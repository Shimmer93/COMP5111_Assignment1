# HKUST COMP5111 (2024Spring) Assignment 2

## Deadline: 11:55 pm 10 May 2024

## Assignment Objective

In assignment 2, you are expected to complete three compulsory tasks and one optional task: 
1. (Task 1) 
   You will get familiar with search-based testing techniques through a state-of-the-art testing tool Evosuite. 
   You are required to use Evosuite to improve the test suite that you constructed in assignment 1. 
   The newly constructed test cases should help improve test coverage for effective fault detection.
2. (Task 2)
   You will implement a fault localization tool on top of Soot. 
   Your tool should be able to locate our injected faults, and you will eventually fix these faults. 
   Besides, you will explore the usage of GenAI (LLMs) in this task and write a report on your usage and findings.
3. (Task 3)
    You will extend the test suite to improve the ranking of fault localization, understanding the limitations of automatically generated tests and the noises in fault localization. Please take advantage of LLMs, and write a report on your usage of LLMs and findings. Through this practice, you will appreciate the characteristics of tests that favor fault localization, as well as the usefulness of LLMs for test generation. 
4. (Bonus Task)
   You are encouraged to study the use of LLMs to generate functional test assertions for the tests generated by Evosuite to *reveal faults* in the *current program version*. We know that Evosuite generates mainly regression assertions. Therefore, tests generated by Evosuite cannot reveal faults of the current program even if their executions fulfill the *inflection* condition. Through this exploration, you will explore the possible combination of the respective advantages of Evosuite and LLMs.

### Having Questions?
1. If you have questions, please first check our [FAQ](Assignment2_FAQ.md). 
2. If your problem is not solved, you are recommended to create `Issues` in this repository.
   Issues shall be attended to by the TA within 24 hrs. You are highly encouraged to start early to accommodate unexpected uncertainties arising from hardware and software.
3. If you want to discuss with other classmates, you can go to `Discussions` in this repository. 
   This repository is `ONLY` for the programming assignment. Note that assignments must be individual work.
   Note: Your reading reports and related discussions for the reading assignment should be made on CANVAS.
4. If you don't want your question to be visible to other classmates, you can send an email to the TA. 

## Assignment Materials

### Class Under Test

The CUT in this assignment is the same as assignment 1.
The class contains (at least two) injected faults.

### Fault-Revealing Tests

We provide six test suites, which contain failing tests that reveal the faults in the CUT.
The fault-revealing test suites can be found in `./src/test/fault-revealing-randoop[0-2]` and `./src/test/fault-revealing-evosuite[0-2]`.
Three of them are generated using `Randoop` and the other three are generated using `Evosuite`. 

#### Environment
- Linux / MacOS / Windows
- Java 11 (SE)
- Eclipse 2023-12
  
#### Libraries
- [Evosuite 1.2.0](https://www.evosuite.org/new-1-2-0-release/)
- [Soot 4.2.1](https://repo1.maven.org/maven2/org/soot-oss/soot/4.2.1/soot-4.2.1-jar-with-dependencies.jar)
- [JUnit 4.12](https://repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar), with [hamcrest-1.3](https://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar) (only needed if you get errors)

If you use `maven`, these dependencies have already been declared in `pom.xml`. In case you want to use `jar` file of the libraries directly, we have already included them in `lib/`.

## Assignment Tasks

### Task 1: Test Case Generation with Evosuite (25%).

#### Requirements
- Use Evosuite to construct **5 test suites** for CUT. 
  The 5 test suites should achieve **higher test coverage** than those generated in assignment 1.
- Each test suite should achieve a coverage as high as possible. 
Hint: You may try different Evosuite parameters to improve coverage.

#### Submissions: 
The 5 test suites generated by you, i.e., the folder `evosuite0`, `evosuite1`, `evosuite2`, `evosuite3`, and `evosuite4` in `src/test`.

For **each suite**, please submit **one screenshot** showing the line coverage and **one screenshot** for branch coverage **captured by EclEmma in Eclipse**, as we did in Assignment 1 Task 1. 
In total, you need to submit a folder containing **10 screenshots**. Each screenshot should be properly named to identify the corresponding line coverage or branch coverage. 
The submission should also include a **readme file** that records the commands (including the parameters) that you used to generate each test suite.

#### Grading Scheme:
1. *Test suites and clear readme files* (5%): Each test suite accounts for `1%` if it can be successfully executed.
2. *Line coverage* (10%): Score = (average line coverage of your test suites / highest average line coverage achieved by your classmates) * 10%
3. *Branch coverage* (10%): Score = (average branch coverage of your test suites / highest average branch coverage achieved by your classmates) * 10%


### Task 2: Fault Localization based on Soot (50%).

In this task, you need to design and implement an effective fault localization algorithm by yourself.

To help you with this task, we provide you with six test suites, located in `./src/test/fault-revealing-randoop[0-2]` and `./src/test/fault-revealing-evosuite[0-2]`.
The program will fail on some tests of these suites. Note that we need at least one failure for fault localization. You need to use these six given test suites for fault localization. Through this exercise, you will appreciate that *generated tests containing noises can affect the effectiveness of fault localization*.

#### Requirements
- Instrument CUT using soot to collect necessary information during the running of tests, and run the **six fault-revealing test suites** we provide to collect the information.
- Implement the classic fault localization algorithm **Ochiai** to compute the fault likelihood of each statement and generate a report for fault localization.
- Localize the faults injected by us based on the spectrum and fix them. Write a brief report to record the found faults and your fixing patches.
- Explore the usefulness of GenAI (LLMs like ChatGPT) for fault localization, and write a report about your usage. You can design a series of effective prompts (as you did in Assignment 1 Task 4) to achieve the goal.

#### Notes
[This](./reference_a2.pdf) research paper describes the **Ochiai** algorithm and can point you to the original papers of the algorithm.

The report should provide useful information indicating where faults reside in the source code. There are **multiple faults** injected in the subject program and each of them is **in single line**. It is possible that some test suites *CANNOT* reveal *ALL* injected faults, but we ensure that each fault that you need to locate and fix must be revealed by *AT LEAST ONE* test suite. In other words, **only the buggy code statements that trigger the failure on our provided tests should be located and fixed**; the remaining code is considered correct and in no need of fault localization and fixing.
We do not tell you the exact number of injected faults, and you need to locate and fix as many faults as you can.

#### Ranking of Suspicious Statements
The ranking of a statement with a suspicious score $a$ should be computed as $\frac{N+M+1}{2}$, where N is the number of statements whose suspicious scores are higher than $a$ and M is the number of statements whose suspicious scores are higher than or equal to $a$. For example, if a sequence of suspicious scores is (0.9, 0.8, 0.8, 0.8, 0.8, 0.7), then their rankings are (1, 3, 3, 3, 3, 6), respectively.

#### Submissions
1. The source code to collect necessary information and generate the spectrum reports. You can build your program upon your code in assignment 1. Scripts to run your program and readme are required.
2. The spectrum reports of potential faulty statements running against **each** of the six test suits provided by us. In total, you need to submit 6 reports. Please name each of them in the format `spectrum_fl_ochiai_randoop[0-2].tsv` and `spectrum_fl_ochiai_evosuite[0-2].tsv`. Each spectrum report should be in `tsv` format and each line should be in the format of "method signature[TAB]statement[TAB]suspicious score[TAB]ranking". The report should be sorted according to the descending order of suspicious scores. If multiple statements have the same scores, please sort them according to the alphabetical order of method signature and statement. The method signature could be obtained using Soot API [getMethod()](https://www.sable.mcgill.ca/soot/doc/soot/Body.html#getMethod()).
3. The fault localization reports of the faults found and fixed by you. For each fault, you should create a `.txt` file and name it in the format `fault_[line-number].txt`, where `line-number` is the line number of the fault you found. In each text file, please
    - Put down its location in Java source code (not in the Jimple file), i.e., the line number and the faulty statement content; 
    - The corresponding fixing patch (note that there can be several ways to fix each bug; we require you to **modify only the faulty line** and *NOT to modify the other code* when fixing each injected bug);
    - The rationale that it is a fault;
    - Its suspicious score and ranking in the reports generated by you.
4. An LLM usage report for this task. You should create only one `.pdf` file and name it as `LLMForTask2.pdf`. Please report:
    - The problem that you use LLMs to solve in your fault localization procedure (e.g., reranking YY, finding faults in XX scope; note that you are *NOT* allowed to simply report the usage of LLMs to help your programming);
    - The prompt that you use to solve the problem;
    - The justification on how your prompt successfully improves the fault localization results (e.g., the ZZ ranking of faults generated LLMs are better than the original; the ABC info generated by LLMs helps me understand PP and guide my QQ). There can be 80-240 words (not a strict limitation, try to be informative but concise).

#### Grading Scheme:

1. *Correctness of your program* (20%): We will check your implementation (10% for a reasonable framework), and the report generated by your program (10% for the correctness of your program's report on a hidden test suite).
2. *Fault Localization & Fix* (20%): If you successfully locate M of our N injected faults, you will earn M/N\*10% score; and if you successfully fix M of N injected faults, you will earn another M/N\*10% score.
3. *Quality of your LLM usage report* (10%): 5% will be earned if you have tried LLMs in the fault localization procedure and recorded the prompt that you use. Another 5% will be earned if your prompt is indeed effective in boosting the fault localization, and you clearly demonstrate the effectiveness of your prompt in your report.

### Task 3: Test Suite Extension (25%).

You may find that the top-ranked suspicious statements reported by your program are not real faults. In such circumstances, you may consider enriching the test suite by adding more test cases manually, with the assistance of LLMs. You can design a series of effective prompts (as you did in Assignment 1 Task 4) to achieve the goal. Through this exercise, you will appreciate the approach of collaborative fault localization, i.e., localizing faults through human-machine collaboration.

#### Requirements
- Extend each of the six given test suites to improve the fault-localization results in Task 2 by adding new tests. 
  - You are *NOT* allowed to delete tests in the six given test suites. You *CAN* use the same set of new tests that you create with LLM's assistance to extend each of the given test suites. That said, you can add the same tests to one or more test suites evosuite/randoop 0/1/2 provided by us simultaneously.
   - The set of new tests that you create must *NOT* contain duplicates, which take the same test input and exercise the same code coverage. That said, you cannot add several same tests to one test suite.
   - `Ochiai` ranking function should be used in ranking suspicious statements. 
- Explore the usefulness of GenAI (LLMs like ChatGPT) for test extension, and write a report about your findings.

#### Submissions

1. The six refined test suites (in Java files). Put them in `./src/test/refined-[randoop|evosuite][0-2]` folder.
2. The spectrum reports of each potential faulty statement running against the refined test suites. In total, you need to submit six reports. Please name each of them in the format `spectrum_fl_ochiai_refined_[randoop|evosuite][0-2].tsv`.
3. A brief extension result report that concisely explains your strategy for test case design and clearly compares the results before and after refinement.
4. An LLM usage report for this task. You should create only one `.pdf` file and name it as `LLMForTask3.pdf`. Please report:
    - The problem that you use LLMs to solve in your test extension procedure (e.g., solving XX constraints and generating tests to cover YY; note that you are *NOT* allowed to simply report the usage of LLMs to help your programming);
    - The prompt that you use to solve the problem;
    - The justification on how your prompt successfully boosts the fault localization procedure (e.g., the ABC info generated by LLMs helps me understand XYZ and get a possible value for PP, and finally guide my QQ). There can be 80-240 words (not a strict limitation, try to be informative but concise).

#### Grading Scheme: 
* *Effectiveness of test cases* (15%). Your grade in this task is related to both your average ranking among all test suites (as mentioned in Task 2) of each injected fault (the *higher*, the better) and the average number of tests you add in each test suite (the *fewer*, the better).

  Your score of each faulty statement `s` is: Score<sub>s</sub> = (15% / N) * (`your average ranking` - `default average ranking`) / (`best average ranking among classmates` - `default average ranking`), where N is the total number of injected faulty statements, and `default average ranking` is the average ranking among all provided test suite of a specific faulty statement.

  Suppose `NT` represents the average number of new tests that you have added to each of the given six test suites. The total grade is given by ΣScore<sub>s</sub> *  (`largest NT among classmates` - `your NT` + 1) / (`largest NT among classmates` - `smallest NT among classmates`)

  **NOTE**: We will run your refined test suites using our fault localization tool with the `Ochiai` ranking function. So your score on Task 3 will not be affected by the correctness of your implementation (which will only be judged in Task 2). 

* *Quality of your LLM usage report* (10%): 5% will be earned if you have tried LLMs in the test extension procedure and recorded the prompt that you use. Another 5% will be earned if your prompt is indeed effective in boosting the test extension, and you clearly demonstrate the effectiveness of your prompt in your report.


### Bonus Task: Applying LLMs to Generate Functional Test Assertion (10% Bonus).

The advantage of Evosuite-generated tests in achieving high coverage is outstanding. However, you should notice that the tests that you generate in Task 1 *CANNOT* reveal the faults that you locate in Task 2. The reason is that Evosuite generates regression tests and the assertion is based on the current buggy version. 
In this task, you will consider finding the correct assertions with LLMs so that the Evosuite-generated call sequence, which triggers a high coverage, can be useful in identifying faults in the current version.
Through this exercise, you will explore a possible solution to combining the respective advantages of Evosuite and LLMs for generating effective tests.

#### Requirements
- Find three tests (NOT suite!) that have the potential to reveal any of the (at least two different) faults that you found in Task 2. That said, such tests will reveal faults when their assertions are properly specified. You need to distinguish such "incorrect but fault revealing" assertions from the other "correct assertions" you generate in Task 1. Note: You should *NOT* use any call sequence that has already been included in the fault-revealing tests provided by us. We will check the similarity of the calling sequence that you choose and the ones in the provided tests.
- Design a series of effective prompts (as you did in Assignment 1 Task 4) that guide LLMs to predict the correct assertion for each test that you selected.
- For each test, combine the LLM-generated assertion with the corresponding Evosuite-generated API call sequence (test driver) into the final tests. *Note that you can **only** modify the assertion and CANNOT modify the call sequences generated by Evosuite.*
- Write a report to explain your prompt strategy that helps you efficiently get the functional assertions and your design experience.

#### Submissions

A report `LLMForBonusTask.pdf` including:

* The three original tests (with original assertions) generated by Evosuite in Task 1 and their corresponding suite name `src/test/evosuite[0-4]` that you choose as the potential tests.
* The assertion generated by LLMs for each of the three chosen tests. For each assertion, please also attach a screenshot of your LLM GUI that proves you successfully prompted the LLMs to generate the attached assertions.
* Three screenshots of Eclipse, of which each shows you run one of the three final tests and successfully trigger and catch the failures, so as to demonstrate the effectiveness of these three final tests.
* Explaining your strategy to design the effective prompt that helps you efficiently get the functional assertions. There can be 80-240 words (not a strict limitation, try to be informative but concise).

#### Grading Scheme: 

* 5% score can be earned once you successfully prepare three fault-revealing tests (with the API call sequence generated by Evosuite and the assertion generated by an LLM).
* Another 5% score can be earned if you present clear and insightful experience in designing effective prompts.

---

## Assignment 2 Submission

- You are required to submit your assignment to [Canvas](https://canvas.ust.hk/courses/54699/assignments).
- Please put all your code, screenshots, readme, and so on into a single folder and compress it to `comp5111asn2-YourStudentID-YourLastName-YourFirstName.zip`

**The *recommended* folder structure is:**

1. Put your code into `${PROJECT_ROOT}/src/main/java/`
2. If you do not use Java build tools, put the libraries jar files that your code depends on into `${PROJECT_ROOT}/lib/`
3. A `README.md` explaining how to run your code. Put your running scripts (if needed) under `${PROJECT_ROOT}/scripts`
4. Put your screenshot into `${PROJECT_ROOT}/screenshots`
5. Put the test suites generated by you into `${PROJECT_ROOT}/src/test/evosuite[0-4]`, and the test suite refined by you into `${PROJECT_ROOT}/src/test/refined-[randoop|evosuite][0-2]`.
6. Put your fault-localization report into the folder that contains the corresponding test suite, e.g., `spectrum_fl_ochiai_randoop0.tsv` in `./src/test/fault-revealing-randoop0`.
7. Put the reports of the faults found and fixed by you (for Task 2) in `${PROJECT_ROOT}/reports/faults`.
8. Put an explanation of your test case refinement strategy (for Task 3) in `${PROJECT_ROOT}/src/test/refined-[randoop|evosuite][0-2]`.
9. Put the LLM usage reports (for Tasks 2, 3, and bonus) in `${PROJECT_ROOT}/reports/LLM`.
